{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "rbj7FNBrxJlY"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "LQnvwleUxJlZ"
      },
      "outputs": [],
      "source": [
        "# Define the KNN class with probability prediction\n",
        "class KNN:\n",
        "    def __init__(self, k=5, distance_metric='euclidean'):\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def compute_distance(self, X1, X2):\n",
        "        if self.distance_metric == 'euclidean':\n",
        "            return np.sqrt(np.sum((X1 - X2) ** 2, axis=1))\n",
        "        elif self.distance_metric == 'manhattan':\n",
        "            return np.sum(np.abs(X1 - X2), axis=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probabilities = [self._predict_proba(x) for x in X]\n",
        "        return np.array(probabilities)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        distances = self.compute_distance(self.X_train, x)\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        return np.bincount(k_nearest_labels).argmax()\n",
        "\n",
        "    def _predict_proba(self, x):\n",
        "        distances = self.compute_distance(self.X_train, x)\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        prob_churn = np.sum(k_nearest_labels) / self.k\n",
        "        return prob_churn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "vohcxlNWxJlZ"
      },
      "outputs": [],
      "source": [
        "# Define data preprocessing function\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# # Preprocess training data function\n",
        "# def preprocess_data(train_path):\n",
        "#     data = pd.read_csv(train_path)\n",
        "#     data = data.drop(columns=['id', 'CustomerId', 'Surname'])\n",
        "#     data = pd.get_dummies(data, columns=['Geography', 'Gender'], drop_first=True)\n",
        "#     X = data.drop(columns=['Exited']).values\n",
        "#     y = data['Exited'].values\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#     scaler = StandardScaler()\n",
        "#     X_train = scaler.fit_transform(X_train)\n",
        "#     X_test = scaler.transform(X_test)\n",
        "#     return X_train, X_test, y_train, y_test, scaler\n",
        "\n",
        "# # Preprocess submission data function\n",
        "# def preprocess_data_submit(submit_path, scaler):\n",
        "#     data = pd.read_csv(submit_path)\n",
        "#     data = data.drop(columns=['id', 'CustomerId', 'Surname'])\n",
        "#     data = pd.get_dummies(data, columns=['Geography', 'Gender'], drop_first=True)\n",
        "#     X = data.values\n",
        "#     X = scaler.transform(X)\n",
        "#     return X\n",
        "\n",
        "\n",
        "# Preprocess data function\n",
        "def preprocess_data(data_path, is_train=True):\n",
        "    # Load the dataset\n",
        "    data = pd.read_csv(data_path)\n",
        "\n",
        "    # Store 'id' column if it exists (for test data)\n",
        "    ids = data['id'] if 'id' in data.columns else None\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    data = data.drop(columns=['id', 'CustomerId', 'Surname']) if is_train else data.drop(columns=['id', 'CustomerId', 'Surname'], errors='ignore')\n",
        "\n",
        "    # Convert categorical columns using One-Hot Encoding\n",
        "    data = pd.get_dummies(data, columns=['Geography', 'Gender'], drop_first=True, dummy_na=False)\n",
        "\n",
        "    # Define X and y\n",
        "    X = data.drop(columns=['Exited'], errors='ignore').values if is_train else data.values  # Ensure X is a NumPy array\n",
        "    y = data['Exited'].values if is_train else None  # Convert y to a NumPy array\n",
        "\n",
        "    return X, y, ids\n",
        "\n",
        "def preprocess_test_data(train_path, test_path):\n",
        "    # Load the train dataset to get the columns and apply preprocessing to the test dataset\n",
        "    train_data = pd.read_csv(train_path)\n",
        "    test_data = pd.read_csv(test_path)\n",
        "\n",
        "    # Store 'id' column if it exists (for test data)\n",
        "    ids = test_data['id']\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    train_data = train_data.drop(columns=['id', 'CustomerId', 'Surname'])\n",
        "    test_data = test_data.drop(columns=['id', 'CustomerId', 'Surname'], errors='ignore')\n",
        "\n",
        "    # Convert categorical columns using One-Hot Encoding, using the same categories as the training data\n",
        "    train_data = pd.get_dummies(train_data, columns=['Geography', 'Gender'], drop_first=True, dummy_na=False)\n",
        "    test_data = pd.get_dummies(test_data, columns=['Geography', 'Gender'], drop_first=True, dummy_na=False)\n",
        "\n",
        "    # Ensure both datasets have the same columns, add missing columns with 0 values\n",
        "    missing_cols = set(train_data.columns) - set(test_data.columns)\n",
        "    for c in missing_cols:\n",
        "        test_data[c] = 0\n",
        "    test_data = test_data[train_data.columns]\n",
        "\n",
        "    # Define X and y (for test data, y is None)\n",
        "    X_test = test_data.drop(columns=['Exited'], errors='ignore').values if 'Exited' in test_data.columns else test_data.values\n",
        "\n",
        "    return X_test, ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "PsZ8I6uhxJlZ"
      },
      "outputs": [],
      "source": [
        "# Define cross-validation function\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Cross-validation function\n",
        "# def cross_validate(X, y, knn, n_splits=5):\n",
        "#     kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "#     auc_scores = []\n",
        "\n",
        "#     for train_index, val_index in kf.split(X):\n",
        "#         # Extract training and validation data using the indices\n",
        "#         X_train, X_val = X[train_index], X[val_index]\n",
        "#         y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "#         # Ensure y_train and y_val are NumPy arrays\n",
        "#         y_train = np.array(y_train)\n",
        "#         y_val = np.array(y_val)\n",
        "\n",
        "#         # Fit the KNN model and predict\n",
        "#         knn.fit(X_train, y_train)\n",
        "#         y_pred = knn.predict(X_val)\n",
        "\n",
        "#         # Calculate AUC score and append\n",
        "#         auc_score = roc_auc_score(y_val, y_pred)\n",
        "#         auc_scores.append(auc_score)\n",
        "\n",
        "#     return auc_scores\n",
        "\n",
        "# Define cross-validation function\n",
        "def cross_validate(X, y, knn, n_splits=5):\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    auc_scores = []\n",
        "    for train_index, val_index in kf.split(X):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "        knn.fit(X_train, y_train)\n",
        "        y_pred = knn.predict(X_val)\n",
        "        auc_score = roc_auc_score(y_val, y_pred)\n",
        "        auc_scores.append(auc_score)\n",
        "    return auc_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igK_w28axJlZ",
        "outputId": "44a5f1cb-7ff0-43e1-cb14-c20b182387bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation AUC scores: [0.7671842387638033, 0.7502202493900786, 0.7783744474920946, 0.7551052631578946, 0.7576418858045185]\n",
            "Average AUC: 0.7617052169216778\n",
            "Test set accuracy: 0.8666666666666667\n",
            "Submission file saved as 'submission.csv' with 10,000 rows.\n"
          ]
        }
      ],
      "source": [
        "# Perform cross-validation (on training set only)\n",
        "cv_scores = cross_validate(X_train, y_train, knn)\n",
        "print(\"Cross-validation AUC scores:\", cv_scores)\n",
        "print(\"Average AUC:\", np.mean(cv_scores))\n",
        "\n",
        "# Evaluate the model on the test set (Optional)\n",
        "test_predictions = knn.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(f\"Test set accuracy: {accuracy}\")\n",
        "\n",
        "# Preprocess the full test data for submission\n",
        "X_submit = preprocess_data_submit('test.csv', scaler)\n",
        "\n",
        "# Make probability predictions for the entire submission data (all 10,000 samples)\n",
        "test_probabilities = knn.predict_proba(X_submit)\n",
        "\n",
        "# Load the test data to extract the 'id' column\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Ensure that the number of predictions matches the number of rows in the test data\n",
        "if len(test_probabilities) != len(test_data):\n",
        "    raise ValueError(f\"Number of predictions ({len(test_probabilities)}) does not match the number of rows in the test data ({len(test_data)}).\")\n",
        "\n",
        "# Create the submission DataFrame with 'id' and 'Exited' columns\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'Exited': test_probabilities  # Predicted probabilities of churn (values between 0 and 1)\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file for submission\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file saved as 'submission.csv' with 10,000 rows.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cs506",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}